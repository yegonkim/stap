{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# with h5py.File(f'data_large/Burgers_train_100000_default.h5', 'r') as f:\n",
    "#     # Traj_dataset.traj_train = torch.tensor(f['train']['pde_140-256'][:10000, :131], dtype=torch.float32, device=cfg.device)\n",
    "#     traj = torch.tensor(f['train']['pde_140-256'][:1000, :131], dtype=torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from generate_data import generate_timestep\n",
    "from tqdm import tqdm\n",
    "\n",
    "hydra.initialize(config_path=\"cfg_time_batch\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"config\", overrides=[\"task=KS\", \"nt=14\"])\n",
    "\n",
    "from utils import set_seed\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0709), tensor(0.1261), tensor(1.3528))\n",
      "0.25\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:43<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.1365), tensor(0.1214), tensor(5.2219))\n",
      "0.5\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.2584), tensor(0.1151), tensor(18.5526))\n",
      "1.0\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:41<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.5377), tensor(0.1195), tensor(81.2457))\n",
      "2.0\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:41<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.5058), tensor(0.1674), tensor(565.1122))\n",
      "4.0\n",
      "Mean: 4.994306079808553e-10, Std: 1.3284176588058472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(6.3953), tensor(0.3545), tensor(8328.8721))\n",
      "max-min scaling\n",
      "Max: 3.5027523040771484, Min: -3.430288553237915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:31<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.1184), tensor(0.0807), tensor(3.7226))\n"
     ]
    }
   ],
   "source": [
    "for scale in [0.125, 0.25, 0.5, 1.0, 2.0, 4.0]:\n",
    "    print(scale)\n",
    "\n",
    "    with h5py.File(cfg.dataset.train_path, 'r') as f:\n",
    "        # Traj_dataset.traj_train = torch.tensor(f['train']['pde_140-256'][:10000, :131], dtype=torch.float32, device=cfg.device)\n",
    "        traj = torch.tensor(f['train']['pde_140-256'][:1000, :131], dtype=torch.float32)\n",
    "\n",
    "    mean = traj[:32].mean()\n",
    "    std = traj[:32].std()\n",
    "    print(f'Mean: {mean}, Std: {std}')\n",
    "    traj = (traj - mean) / std * scale\n",
    "\n",
    "    from neuralop.models import FNO\n",
    "\n",
    "    unrolling = cfg.train.unrolling\n",
    "    nt = cfg.nt\n",
    "    ensemble_size = cfg.ensemble_size\n",
    "    num_acquire = cfg.num_acquire\n",
    "    device = cfg.device\n",
    "    epochs = cfg.train.epochs\n",
    "    lr = cfg.train.lr\n",
    "    batch_size = cfg.train.batch_size\n",
    "    initial_datasize = cfg.initial_datasize\n",
    "\n",
    "    def train(Y, train_nts, **kwargs):\n",
    "        model = FNO(n_modes=(256, ), hidden_channels=64,\n",
    "                    in_channels=1, out_channels=1)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for b in range(Y.shape[0]):\n",
    "            for t in range(train_nts[b].item()-1):\n",
    "                inputs.append(Y[b,t])\n",
    "                outputs.append(Y[b, t+1])\n",
    "        inputs = torch.stack(inputs, dim=0).unsqueeze(1)\n",
    "        outputs = torch.stack(outputs, dim=0).unsqueeze(1)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            model.train()\n",
    "            # max_unrolling = epoch if epoch <= unrolling else unrolling\n",
    "            # unrolling_list = [r for r in range(max_unrolling + 1)]\n",
    "\n",
    "            total_loss = 0\n",
    "            for x, y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "                # loss = torch.sqrt(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            # wandb.log({f'train/loss_{acquire_step}': total_loss})\n",
    "        return model\n",
    "\n",
    "    timestep = (traj.shape[1] - 1) // (nt - 1) # 10\n",
    "    Y = traj[:,0::timestep]\n",
    "    train_nts = torch.ones(Y.shape[0], device=device, dtype=torch.int64)\n",
    "    train_nts[:initial_datasize] = nt\n",
    "\n",
    "    model = train(Y, train_nts)\n",
    "\n",
    "    from utils import torch_expand\n",
    "    from eval_utils import compute_metrics\n",
    "\n",
    "    test_Y = traj[initial_datasize:1000, 0::timestep]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    preds.append(test_Y[:,0:1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(nt-1):\n",
    "            X = preds[-1].to(device)\n",
    "            pred = model(X) # batch x 1 x 256\n",
    "            preds.append(pred.cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=1) # batch x nt x 256\n",
    "\n",
    "\n",
    "    metrics = compute_metrics(test_Y, preds, d=2, device=device, reduction=True)\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "print('max-min scaling')\n",
    "\n",
    "with h5py.File(cfg.dataset.train_path, 'r') as f:\n",
    "    # Traj_dataset.traj_train = torch.tensor(f['train']['pde_140-256'][:10000, :131], dtype=torch.float32, device=cfg.device)\n",
    "    traj = torch.tensor(f['train']['pde_140-256'][:1000, :131], dtype=torch.float32)\n",
    "\n",
    "max = traj.max()\n",
    "min = traj.min()\n",
    "print(f'Max: {max}, Min: {min}')\n",
    "traj = (traj - (max+min/2)) / (max - min)\n",
    "\n",
    "from neuralop.models import FNO\n",
    "\n",
    "unrolling = cfg.train.unrolling\n",
    "nt = cfg.nt\n",
    "ensemble_size = cfg.ensemble_size\n",
    "num_acquire = cfg.num_acquire\n",
    "device = cfg.device\n",
    "epochs = cfg.train.epochs\n",
    "lr = cfg.train.lr\n",
    "batch_size = cfg.train.batch_size\n",
    "initial_datasize = cfg.initial_datasize\n",
    "\n",
    "def train(Y, train_nts, **kwargs):\n",
    "    model = FNO(n_modes=(256, ), hidden_channels=64,\n",
    "                in_channels=1, out_channels=1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for b in range(Y.shape[0]):\n",
    "        for t in range(train_nts[b].item()-1):\n",
    "            inputs.append(Y[b,t])\n",
    "            outputs.append(Y[b, t+1])\n",
    "    inputs = torch.stack(inputs, dim=0).unsqueeze(1)\n",
    "    outputs = torch.stack(outputs, dim=0).unsqueeze(1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        # max_unrolling = epoch if epoch <= unrolling else unrolling\n",
    "        # unrolling_list = [r for r in range(max_unrolling + 1)]\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # loss = torch.sqrt(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # wandb.log({f'train/loss_{acquire_step}': total_loss})\n",
    "    return model\n",
    "\n",
    "timestep = (traj.shape[1] - 1) // (nt - 1) # 10\n",
    "Y = traj[:,0::timestep]\n",
    "train_nts = torch.ones(Y.shape[0], device=device, dtype=torch.int64)\n",
    "train_nts[:initial_datasize] = nt\n",
    "\n",
    "model = train(Y, train_nts)\n",
    "\n",
    "from utils import torch_expand\n",
    "from eval_utils import compute_metrics\n",
    "\n",
    "test_Y = traj[initial_datasize:1000, 0::timestep]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "preds.append(test_Y[:,0:1])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(nt-1):\n",
    "        X = preds[-1].to(device)\n",
    "        pred = model(X) # batch x 1 x 256\n",
    "        preds.append(pred.cpu())\n",
    "\n",
    "preds = torch.cat(preds, dim=1) # batch x nt x 256\n",
    "\n",
    "\n",
    "metrics = compute_metrics(test_Y, preds, d=2, device=device, reduction=True)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
