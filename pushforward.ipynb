{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# with h5py.File(f'data_large/Burgers_train_100000_default.h5', 'r') as f:\n",
    "#     # Traj_dataset.traj_train = torch.tensor(f['train']['pde_140-256'][:10000, :131], dtype=torch.float32, device=cfg.device)\n",
    "#     traj = torch.tensor(f['train']['pde_140-256'][:1000, :131], dtype=torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from generate_data import generate_timestep\n",
    "from tqdm import tqdm\n",
    "\n",
    "hydra.initialize(config_path=\"cfg_time_batch\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"config\", overrides=[\"task=Heat\", \"nt=131\", \"initial_datasize=64\"])\n",
    "\n",
    "from utils import set_seed\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 2.542020320892334, Min: -2.45027232170105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:38<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0016), tensor(0.0040), tensor(0.0890))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:08<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0060), tensor(0.0152), tensor(0.6709))\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from utils import torch_expand\n",
    "from eval_utils import compute_metrics\n",
    "\n",
    "with h5py.File(cfg.dataset.train_path, 'r') as f:\n",
    "    # Traj_dataset.traj_train = torch.tensor(f['train']['pde_140-256'][:10000, :131], dtype=torch.float32, device=cfg.device)\n",
    "    traj = torch.tensor(f['train']['pde_140-256'][:1000, :131], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# mean = traj[:32].mean()\n",
    "# std = traj[:32].std()\n",
    "# print(f'Mean: {mean}, Std: {std}')\n",
    "# traj = (traj - mean) / std\n",
    "\n",
    "\n",
    "max = traj.max()\n",
    "min = traj.min()\n",
    "print(f'Max: {max}, Min: {min}')\n",
    "traj = (traj - (max+min/2)) / (max - min)\n",
    "\n",
    "from neuralop.models import FNO\n",
    "\n",
    "nt = cfg.nt\n",
    "ensemble_size = cfg.ensemble_size\n",
    "num_acquire = cfg.num_acquire\n",
    "device = cfg.device\n",
    "epochs = cfg.train.epochs\n",
    "lr = cfg.train.lr\n",
    "batch_size = cfg.train.batch_size\n",
    "initial_datasize = cfg.initial_datasize\n",
    "\n",
    "def train(Y, train_nts, **kwargs):\n",
    "    model = FNO(n_modes=(256, ), hidden_channels=64,\n",
    "                in_channels=1, out_channels=1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for b in range(Y.shape[0]):\n",
    "        for t in range(train_nts[b].item()-1):\n",
    "            inputs.append(Y[b,t])\n",
    "            outputs.append(Y[b, t+1])\n",
    "    inputs = torch.stack(inputs, dim=0).unsqueeze(1)\n",
    "    outputs = torch.stack(outputs, dim=0).unsqueeze(1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        # max_unrolling = epoch if epoch <= unrolling else unrolling\n",
    "        # unrolling_list = [r for r in range(max_unrolling + 1)]\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # loss = torch.sqrt(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # wandb.log({f'train/loss_{acquire_step}': total_loss})\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_pushforward(Y, train_nts, unrolling, **kwargs):\n",
    "    model = FNO(n_modes=(256, ), hidden_channels=64,\n",
    "                in_channels=1, out_channels=1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    datasize = (train_nts-1).sum().item()\n",
    "    average_iter = datasize // batch_size\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    unroll = []\n",
    "    for b in range(Y.shape[0]):\n",
    "        for t in range(train_nts[b].item()-1):\n",
    "            for r in range(unrolling):\n",
    "                if t+(r+1) < train_nts[b].item():\n",
    "                    inputs.append(Y[b,t])\n",
    "                    outputs.append(Y[b, t+(r+1)])\n",
    "                    unroll.append(r)\n",
    "    inputs = torch.stack(inputs, dim=0).unsqueeze(1)\n",
    "    outputs = torch.stack(outputs, dim=0).unsqueeze(1)\n",
    "    unrolling = torch.tensor(unroll, device=device)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(inputs, outputs, unrolling)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        # max_unrolling = epoch if epoch <= unrolling else unrolling\n",
    "        # unrolling_list = [r for r in range(max_unrolling + 1)]\n",
    "\n",
    "        total_loss = 0\n",
    "        # for x, y, unroll in islice(dataloader, 0, average_iter):\n",
    "        for x, y, unroll in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            for _ in range(unroll.max()):\n",
    "                with torch.no_grad():\n",
    "                    x[unroll > 0] = model(x[unroll > 0])\n",
    "                    unroll[unroll > 0] -= 1\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # loss = torch.sqrt(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # wandb.log({f'train/loss_{acquire_step}': total_loss})\n",
    "    return model\n",
    "\n",
    "def train_gaussian(Y, train_nts, noise_std=0.01, **kwargs):\n",
    "    model = FNO(n_modes=(256, ), hidden_channels=64,\n",
    "                in_channels=1, out_channels=1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for b in range(Y.shape[0]):\n",
    "        for t in range(train_nts[b].item()-1):\n",
    "            inputs.append(Y[b,t])\n",
    "            outputs.append(Y[b, t+1])\n",
    "    inputs = torch.stack(inputs, dim=0).unsqueeze(1)\n",
    "    outputs = torch.stack(outputs, dim=0).unsqueeze(1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        # max_unrolling = epoch if epoch <= unrolling else unrolling\n",
    "        # unrolling_list = [r for r in range(max_unrolling + 1)]\n",
    "\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                x = x + noise_std * torch.randn_like(x)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # loss = torch.sqrt(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # wandb.log({f'train/loss_{acquire_step}': total_loss})\n",
    "    return model\n",
    "\n",
    "def evaluate():\n",
    "    test_Y = traj[initial_datasize:1000, 0::timestep]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    preds.append(test_Y[:,0:1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(nt-1):\n",
    "            X = preds[-1].to(device)\n",
    "            pred = model(X) # batch x 1 x 256\n",
    "            preds.append(pred.cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=1) # batch x nt x 256\n",
    "\n",
    "\n",
    "    metrics = compute_metrics(test_Y, preds, d=2, device=device, reduction=True)\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "timestep = (traj.shape[1] - 1) // (nt - 1) # 10\n",
    "Y = traj[:,0::timestep]\n",
    "train_nts = torch.ones(Y.shape[0], device=device, dtype=torch.int64)\n",
    "train_nts[:initial_datasize] = nt\n",
    "\n",
    "# model = train(Y, train_nts)\n",
    "# evaluate()\n",
    "\n",
    "model = train_pushforward(Y, train_nts, 1)\n",
    "evaluate()\n",
    "\n",
    "model = train_gaussian(Y, train_nts, noise_std=0.01)\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
